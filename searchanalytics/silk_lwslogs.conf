input {
	file {
		type => "lwscorelog"
		# FYI - LogStash does not always recurse the directory hierarchy correctly on Windows
		# unless the path is all lowercase.
		path => [ "/Users/ravikrishnamurthy/Documents/src/demo_log_generator/mysamplelogs/lucidfind/core-logs/core.2013*" ]
# core request logs have different information and  format
		exclude => ["*.gz","*.zip","*.tgz", "*request*"]
		sincedb_path => "/Users/ravikrishnamurthy/Documents/demos/slk-4.7.0/solrWriterForLogStash/logstash_deploy/LucidFind.sincedb"
    	start_position => "beginning"
   }
}
filter {
	if [type] == "lwscorelog" {
	
		
		grep {
			ignore_case => true
			negate => true
			match => ["message", "\[lucidfindlogs\]"]
			match => ["message", "\[LucidWorksLogs\]"]
		}
		
		#grep {
		#	ignore_case => true
		#	match => ["message", "path=/browse"]
		#}
		
		grok {
			match => ["message", "%{TIMESTAMP_ISO8601:received_at} INFO %{DATA} \[%{DATA:collection}\] webapp=%{DATA:webapp} path=%{DATA:searchhandler} params={%{DATA}q=%{DATA:queryterms}[&}]%{DATA} hits=%{BASE10NUM:hits} status=%{BASE10NUM:status} QTime=%{BASE10NUM:qtime}"]
		}

		if ("_grokparsefailure" in [tags]) {
      		drop{}
    		}
	
	
		date {
		# Pull the time stamp from the 'received_at' field (parsed above with grok)
		# date filter will write it into the @timestamp field, which the solr output writer will convert based on the field prefix
			match => [ "received_at", "yyyy-MM-dd HH:mm:ss,SSS" ]
		}	
	
	}
}	


output {
	stdout { debug => true codec => "rubydebug"}
	lucidworks_solr_lsv133 { collection_host => "localhost" collection_port => "8888" collection_name => "lwssearchlogs" field_prefix => "event_" force_commit => false flush_size => 100 idle_flush_time => 1 }
}


